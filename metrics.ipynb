{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def calculate_total_loans_per_school(df):\n",
    "    total_loans = df.groupBy(\"SchoolName\").agg(\n",
    "        F.sum(\"ffel_subsidized_amount_of_loans_originated\").alias(\"Total_Subsidized\"),\n",
    "        F.sum(\"ffel_unsubsidized_amount_of_loans_originated\").alias(\"Total_Unsubsidized\"),\n",
    "        F.sum(\"ffel_stafford_amount_of_loans_originated\").alias(\"Total_Stafford\"),\n",
    "        F.sum(\"ffel_plus_amount_of_loans_originated\").alias(\"Total_PLUS\")\n",
    "    )\n",
    "    total_loans = total_loans.withColumn(\n",
    "        \"Total_Loans\",\n",
    "        F.expr(\"Total_Subsidized + Total_Unsubsidized + Total_Stafford + Total_PLUS\")\n",
    "    )\n",
    "    return total_loans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loans_per_state(df):\n",
    "    total_loans_state = df.groupBy(\"State\").agg(\n",
    "        F.sum(\"ffel_subsidized_amount_of_loans_originated\").alias(\"Total_Subsidized\"),\n",
    "        F.sum(\"ffel_unsubsidized_amount_of_loans_originated\").alias(\"Total_Unsubsidized\"),\n",
    "        F.sum(\"ffel_stafford_amount_of_loans_originated\").alias(\"Total_Stafford\"),\n",
    "        F.sum(\"ffel_plus_amount_of_loans_originated\").alias(\"Total_PLUS\")\n",
    "    )\n",
    "    total_loans_state = total_loans_state.withColumn(\n",
    "        \"Total_Loans\",\n",
    "        F.expr(\"Total_Subsidized + Total_Unsubsidized + Total_Stafford + Total_PLUS\")\n",
    "    )\n",
    "    return total_loans_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loan_evolution(df):\n",
    "    loan_evolution = df.groupBy(\"Quarter_Start\", \"Quarter_End\").agg(\n",
    "        F.sum(\"ffel_subsidized_amount_of_loans_originated\").alias(\"Total_Subsidized\"),\n",
    "        F.sum(\"ffel_unsubsidized_amount_of_loans_originated\").alias(\"Total_Unsubsidized\"),\n",
    "        F.sum(\"ffel_stafford_amount_of_loans_originated\").alias(\"Total_Stafford\"),\n",
    "        F.sum(\"ffel_plus_amount_of_loans_originated\").alias(\"Total_PLUS\")\n",
    "    )\n",
    "    loan_evolution = loan_evolution.withColumn(\n",
    "        \"Total_Loans\",\n",
    "        F.expr(\"Total_Subsidized + Total_Unsubsidized + Total_Stafford + Total_PLUS\")\n",
    "    )\n",
    "    return loan_evolution.orderBy(\"Quarter_Start\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def save_metrics(df, path, source_path, backup_dir,partition_by=None):\n",
    "    current_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    backup_path = f\"{backup_dir}/dfparquet_backup_{current_timestamp}\"\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"DataBackup\").getOrCreate()\n",
    "    spark.conf.set(\"spark.sql.files.ignoreMissingFiles\", \"true\")\n",
    "\n",
    "    # Utiliser l'API Hadoop FileSystem pour copier le fichier\n",
    "    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "\n",
    "    # Importer URI depuis Java\n",
    "    URI = spark._jvm.java.net.URI\n",
    "\n",
    "    # Obtenir le FileSystem HDFS en spécifiant l'URI\n",
    "    hdfs_uri = URI(\"hdfs://localhost:9080\")\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hdfs_uri, hadoop_conf)\n",
    "\n",
    "    # Créer les objets Path\n",
    "    source = spark._jvm.org.apache.hadoop.fs.Path(source_path)\n",
    "    destination = spark._jvm.org.apache.hadoop.fs.Path(backup_path)\n",
    "    backup_dir_path = spark._jvm.org.apache.hadoop.fs.Path(backup_dir)\n",
    "\n",
    "    # Vérifier si le répertoire de sauvegarde existe, sinon le créer\n",
    "    if not fs.exists(backup_dir_path):\n",
    "        fs.mkdirs(backup_dir_path)\n",
    "\n",
    "    # Copier le fichier au sein de HDFS\n",
    "    FileUtil = spark._jvm.org.apache.hadoop.fs.FileUtil\n",
    "    FileUtil.copy(fs, source, fs, destination, False, hadoop_conf)\n",
    "    \n",
    "    if partition_by:\n",
    "        df.write.mode(\"overwrite\").partitionBy(partition_by).parquet(path)\n",
    "    else:\n",
    "        df.write.mode(\"overwrite\").parquet(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/22 18:04:19 WARN Utils: Your hostname, MacBook-Air-de-Anthony.local resolves to a loopback address: 127.0.0.1; using 192.168.1.30 instead (on interface en0)\n",
      "24/11/22 18:04:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/22 18:04:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------------------+--------------+----------+-----------+\n",
      "|          SchoolName|Total_Subsidized|Total_Unsubsidized|Total_Stafford|Total_PLUS|Total_Loans|\n",
      "+--------------------+----------------+------------------+--------------+----------+-----------+\n",
      "|   HUMPHREYS COLLEGE|         5145756|           7490527|        188187|     37852|   12862322|\n",
      "|UNIVERSITY OF ALA...|         8637357|          11716884|        325886|     75620|   20755747|\n",
      "|UNIVERSITY OF OXF...|           17000|             23500|         39924|     28300|     108724|\n",
      "|UNIVERSITY OF NEW...|           94610|            113640|         87803|    116011|     412064|\n",
      "|ANCILLA DOMINI CO...|         1588504|           1883644|        222711|         0|    3694859|\n",
      "+--------------------+----------------+------------------+--------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/22 18:04:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"CalculateMetrics\").getOrCreate()\n",
    "\n",
    "    # Lire le DataFrame depuis HDFS\n",
    "    hdfs_parquet_path = \"hdfs://localhost:9080/user/anthonycormeaux/data/result/joined_data\"\n",
    "    df = spark.read.parquet(hdfs_parquet_path)\n",
    "\n",
    "    # Calculer les métriques\n",
    "    total_loans_per_school = calculate_total_loans_per_school(df)\n",
    "    total_loans_per_state = calculate_total_loans_per_state(df)\n",
    "    loan_evolution = calculate_loan_evolution(df)\n",
    "\n",
    "    print(total_loans_per_school.show(5))\n",
    "\n",
    "    # Stocker les résultats\n",
    "    save_metrics(total_loans_per_school, path=\"hdfs://localhost:9080/user/anthonycormeaux/data/total_loans_per_school\", source_path=\"hdfs://localhost:9080/user/anthonycormeaux/data/total_loans_per_school\", backup_dir=\"hdfs://localhost:9080/user/anthonycormeaux/data/metrics_backup/total_loans_per_school\")\n",
    "    save_metrics(total_loans_per_state, path=\"hdfs://localhost:9080/user/anthonycormeaux/data/total_loans_per_state\", source_path=\"hdfs://localhost:9080/user/anthonycormeaux/data/total_loans_per_state\", backup_dir=\"hdfs://localhost:9080/user/anthonycormeaux/data/metrics_backup/total_loans_per_state\")\n",
    "    save_metrics(loan_evolution, path=\"hdfs://localhost:9080/user/anthonycormeaux/data/loan_evolution\", source_path=\"hdfs://localhost:9080/user/anthonycormeaux/data/loan_evolution\", backup_dir=\"hdfs://localhost:9080/user/anthonycormeaux/data/metrics_backup/loan_evolution\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
