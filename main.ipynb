{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyhdfs\n",
    "import logging\n",
    "from config import hdfs_config\n",
    "\n",
    "HDFS_HOST = 'localhost'  # Replace with your HDFS host\n",
    "HDFS_PORT = 9870         # Default WebHDFS port\n",
    "HDFS_RAW_DEST_PATH = '/user/hadoop/data/raw'  # Destination path in HDFS\n",
    "HDFS_CLEAN_DEST_PATH = '/user/hadoop/data/cleaned'  # Destination path in HDFS\n",
    "LOCAL_RAW_DATA_PATH = '/home/freddy/Documents/Cours_efrei/Data_integration/Projets_data_integration/data_integration_student_loans/data/raw'\n",
    "\n",
    "\n",
    "def upload_files_to_hdfs(local_path, hdfs_path, hdfs_client):\n",
    "    \"\"\"Uploads files from a local directory to HDFS with detailed error handling.\"\"\"\n",
    "    try:\n",
    "        # Ensure the HDFS directory exists\n",
    "        if not hdfs_client.exists(hdfs_path):\n",
    "            hdfs_client.mkdirs(hdfs_path)\n",
    "            logging.info(f\"Created HDFS directory: {hdfs_path}\")\n",
    "        \n",
    "        # Check if the local directory exists\n",
    "        if not os.path.exists(local_path):\n",
    "            logging.error(f\"Local directory {local_path} does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Iterate through files in the local raw data directory\n",
    "        for file_name in os.listdir(local_path):\n",
    "            local_file_path = os.path.join(local_path, file_name)\n",
    "\n",
    "            # Only process files\n",
    "            if os.path.isfile(local_file_path):\n",
    "                hdfs_file_path = f\"{hdfs_path}/{file_name}\"\n",
    "\n",
    "                # Check if the file already exists in HDFS\n",
    "                if hdfs_client.exists(hdfs_file_path):\n",
    "                    logging.info(f\"File {hdfs_file_path} already exists in HDFS. Skipping upload.\")\n",
    "                    continue\n",
    "\n",
    "                # Attempt to upload file to HDFS\n",
    "                try:\n",
    "                    with open(local_file_path, 'rb') as file_data:\n",
    "                        hdfs_client.create(hdfs_file_path, file_data)\n",
    "                    logging.info(f\"Uploaded {file_name} to HDFS at {hdfs_file_path}\")\n",
    "\n",
    "                    # Verify upload success\n",
    "                    if hdfs_client.exists(hdfs_file_path):\n",
    "                        logging.info(f\"Successfully uploaded: {file_name}\")\n",
    "                    else:\n",
    "                        logging.error(f\"Upload verification failed: {file_name}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to upload {file_name} to HDFS due to {e}\")\n",
    "\n",
    "            else:\n",
    "                logging.warning(f\"{local_file_path} is not a file. Skipping.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during file upload: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize HDFS client\n",
    "    hdfs_client = pyhdfs.HdfsClient(hosts=f\"{HDFS_HOST}:{HDFS_PORT}\")\n",
    "    logging.info(\"HDFS client initialized\")\n",
    "    \n",
    "    # Start uploading files\n",
    "    upload_files_to_hdfs(LOCAL_RAW_DATA_PATH, HDFS_RAW_DEST_PATH, hdfs_client)\n",
    "    \n",
    "except pyhdfs.HdfsException as he:\n",
    "    logging.error(f\"HDFS error: Failed to initialize HDFS client or perform operations due to {he}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Unexpected error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from config.kafka_config import KAFKA_BROKER_URL, TOPIC_NAME_FL_DASHBOARD, BATCH_SIZE, POLL_INTERVAL\n",
    "\n",
    "# Initialize Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BROKER_URL,\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')  # Serialize data to JSON\n",
    ")\n",
    "\n",
    "def read_data_in_batches(file_path, batch_size):\n",
    "    \"\"\"Reads data from file in batches.\"\"\"\n",
    "    \"\"\"Reads data from an Excel file in batches.\"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        yield df[start:start + batch_size].to_dict(orient='records')\n",
    "\n",
    "def send_data_to_kafka(topic, file_path):\n",
    "    \"\"\"Sends data from the file to Kafka in batches.\"\"\"\n",
    "    for batch in read_data_in_batches(file_path, BATCH_SIZE):\n",
    "        for record in batch:\n",
    "            producer.send(topic, value=record)  # Send each record to Kafka\n",
    "        producer.flush()  # Ensure data is sent before sleeping\n",
    "        print(f\"Sent batch of {len(batch)} records to Kafka topic '{topic}'\")\n",
    "        time.sleep(POLL_INTERVAL)\n",
    "\n",
    "def main():\n",
    "    # Path to raw data files\n",
    "    raw_data_dir = os.path.join(\"data\", \"raw\")\n",
    "    \n",
    "    # Send FL_Dashboard files\n",
    "    for filename in os.listdir(raw_data_dir):\n",
    "        if filename.startswith(\"FL_Dashboard\"):\n",
    "            file_path = os.path.join(raw_data_dir, filename)\n",
    "            print(f\"Streaming data from file: {file_path}\")\n",
    "            send_data_to_kafka(TOPIC_NAME_FL_DASHBOARD, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data from file: data/raw/FL_Dashboard_AY2009_2010_Q1.xlsx\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 25 records to Kafka topic 'fl_dashboard_topic'\n",
      "Streaming data from file: data/raw/FL_Dashboard_AY2009_2010_Q2.xlsx\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n",
      "Sent batch of 100 records to Kafka topic 'fl_dashboard_topic'\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka consumer to pull data from Kafka\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from config.kafka_config import KAFKA_BROKER_URL, TOPIC_NAME_FL_DASHBOARD, TOPIC_NAME_SCHOOL_CODELIST\n",
    "from config.spark_config import APP_NAME, MASTER, BATCH_DURATION\n",
    "import json\n",
    "\n",
    "def start_spark_streaming():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(APP_NAME) \\\n",
    "        .master(MASTER) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    # Read from Kafka\n",
    "    df_kafka = spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKER_URL) \\\n",
    "        .option(\"subscribe\", f\"{TOPIC_NAME_FL_DASHBOARD},{TOPIC_NAME_SCHOOL_CODELIST}\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "    # Convert Kafka's binary data to string\n",
    "    df_kafka = df_kafka.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "    # Define schema or processing functions as needed for incoming data\n",
    "    # This example assumes JSON strings; adjust according to your data format\n",
    "    schema = \"your_schema_here\"  # Define your schema here\n",
    "\n",
    "    df_parsed = df_kafka.withColumn(\"value\", from_json(col(\"value\"), schema))\n",
    "\n",
    "    # Write the parsed data to the console (for debugging)\n",
    "    query = df_parsed.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "\n",
    "    # Wait for the termination signal\n",
    "    query.awaitTermination()\n",
    "\n",
    "\n",
    "start_spark_streaming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
