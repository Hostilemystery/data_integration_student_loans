{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import hdfs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_HOST = hdfs_config.HDFSConfig.HOST\n",
    "HDFS_PORT = hdfs_config.HDFSConfig.PORT\n",
    "LOCAL_RAW_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_RAW_DATA_PATH\n",
    "HDFS_RAW_DEST_PATH = hdfs_config.HDFSConfig.RAW_DEST_PATH\n",
    "HDFS_CLEAN_DEST_PATH = hdfs_config.HDFSConfig.CLEAN_DEST_PATH\n",
    "LOCAL_CLEAN_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_CLEAN_DATA_PATH\n",
    "\n",
    "LOCAL_COMBINED_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_COMBINED_DATA_PATH\n",
    "COMBINED_DEST_PATH = hdfs_config.HDFSConfig.COMBINED_DEST_PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rm/47tt4n7s30533krtvlkg53340000gn/T/ipykernel_89165/1375525828.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rm/47tt4n7s30533krtvlkg53340000gn/T/ipykernel_89165/1375525828.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rm/47tt4n7s30533krtvlkg53340000gn/T/ipykernel_89165/1375525828.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/arrow/array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rm/47tt4n7s30533krtvlkg53340000gn/T/ipykernel_89165/1375525828.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/arrow/array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/bottleneck/__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/22 18:00:51 WARN Utils: Your hostname, MacBook-Air-de-Anthony.local resolves to a loopback address: 127.0.0.1; using 192.168.1.30 instead (on interface en0)\n",
      "24/11/22 18:00:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/22 18:00:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting quarter dates: Excel file format cannot be determined, you must specify an engine manually.\n",
      "Error loading data from /Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/raw/~$FL_Dashboard_AY2009_2010_Q1.xlsx: Excel file format cannot be determined, you must specify an engine manually.\n",
      "Error processing ~$FL_Dashboard_AY2009_2010_Q1.xlsx: 'tuple' object has no attribute 'withColumn'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/22 18:00:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned data to /Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned/cleaned_FL_Dashboard_AY2009_2010_Q1.xlsx\n",
      "Saved cleaned data to /Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned/cleaned_FL_Dashboard_AY2009_2010_Q2.xlsx\n",
      "Converted output file to .xlsx format: /Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned/cleaned_FL_Dashboard_AY2009_2010_Q3.xlsx\n",
      "Saved cleaned data to /Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned/cleaned_FL_Dashboard_AY2009_2010_Q3.xlsx\n",
      "Converted output file to .xlsx format: /Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned/cleaned_FL_Dashboard_AY2009_2010_Q4.xlsx\n",
      "Saved cleaned data to /Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned/cleaned_FL_Dashboard_AY2009_2010_Q4.xlsx\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    return SparkSession.builder.appName(\"Data Cleaning\").getOrCreate()\n",
    "\n",
    "# Extract quarter start and end dates from Quarterly Activity sheet (specific to FFEL Dashboard files)\n",
    "def extract_quarter_dates(file_path):\n",
    "    try:\n",
    "        metadata_df = pd.read_excel(file_path, sheet_name='Quarterly Activity', nrows=5)\n",
    "        metadata_text = \" \".join(metadata_df.astype(str).values.flatten())\n",
    "        quarter_dates_match = re.search(r\"\\((\\d{2}/\\d{2}/\\d{4})-(\\d{2}/\\d{2}/\\d{4})\\)\", metadata_text)\n",
    "        quarter_start = quarter_dates_match.group(1) if quarter_dates_match else None\n",
    "        quarter_end = quarter_dates_match.group(2) if quarter_dates_match else None\n",
    "        return quarter_start, quarter_end\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting quarter dates: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load FFEL Dashboard data from Excel sheets and convert to Spark DataFrames\n",
    "def load_ffel_data(file_path, spark):\n",
    "    try:\n",
    "        quarterly_activity_pd = pd.read_excel(file_path, sheet_name='Quarterly Activity', skiprows=5, dtype={'OPE ID': str})\n",
    "        #award_year_summary_pd = pd.read_excel(file_path, sheet_name='Award Year Summary', skiprows=5, dtype={'OPE ID': str})\n",
    "        quarterly_activity_spark = spark.createDataFrame(quarterly_activity_pd)\n",
    "        #award_year_summary_spark = spark.createDataFrame(award_year_summary_pd)\n",
    "        return quarterly_activity_spark #, award_year_summary_spark\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Add quarter dates for FFEL data\n",
    "def add_quarter_dates(quarterly_df, quarter_start, quarter_end):\n",
    "    #add_quarter_dates(quarterly_df, award_year_df, quarter_start, quarter_end):\n",
    "    quarterly_df = quarterly_df.withColumn(\"Quarter_Start\", lit(quarter_start)).withColumn(\"Quarter_End\", lit(quarter_end))\n",
    "    #award_year_df = award_year_df.withColumn(\"Quarter_Start\", lit(None)).withColumn(\"Quarter_End\", lit(None))\n",
    "    return quarterly_df#, award_year_df\n",
    "\n",
    "# Combine DataFrames and remove duplicates\n",
    "def combine_dataframes(*dfs):\n",
    "    combined_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        combined_df = combined_df.union(df)\n",
    "    return combined_df.dropDuplicates()\n",
    "\n",
    "# Rename columns to a standardized format\n",
    "def rename_columns(df, column_mapping):\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df.columns:\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "    return df\n",
    "\n",
    "# Convert specified columns to double type\n",
    "def cast_columns_to_double(df, columns_to_cast):\n",
    "    for col_name in columns_to_cast:\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "    return df\n",
    "\n",
    "# Save the cleaned DataFrame to Excel\n",
    "def save_to_excel(df, output_path):\n",
    "    try:\n",
    "        # Ensure output is saved in `.xlsx` format for compatibility\n",
    "        if output_path.endswith(\".xls\"):\n",
    "            output_path = output_path.replace(\".xls\", \".xlsx\")\n",
    "            print(f\"Converted output file to .xlsx format: {output_path}\")\n",
    "\n",
    "        df_pd = df.toPandas()\n",
    "        df_pd.to_excel(output_path, index=False, float_format=\"%.2f\")\n",
    "        print(f\"Saved cleaned data to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "# Main function to process FFEL data files\n",
    "def process_ffel_data(file_path, output_path,spark):\n",
    "    \n",
    "    \n",
    "    # Extract quarter dates\n",
    "    quarter_start, quarter_end = extract_quarter_dates(file_path)\n",
    "    \n",
    "    # Load FFEL data\n",
    "    quarterly_activity_spark = load_ffel_data(file_path, spark)\n",
    "    #quarterly_activity_spark, award_year_summary_spark = load_ffel_data(file_path, spark)\n",
    "    if not quarterly_activity_spark :\n",
    "    #if not quarterly_activity_spark or not award_year_summary_spark:\n",
    "        print(f\"Skipping {file_path} due to load errors.\")\n",
    "        return\n",
    "    \n",
    "    # Add quarter dates\n",
    "    quarterly_activity_spark = add_quarter_dates(\n",
    "        quarterly_activity_spark, quarter_start, quarter_end\n",
    "    )\n",
    "    # quarterly_activity_spark, award_year_summary_spark = add_quarter_dates(\n",
    "    #     quarterly_activity_spark, award_year_summary_spark, quarter_start, quarter_end\n",
    "    # )\n",
    "    \n",
    "    # Combine data\n",
    "    combined_df = combine_dataframes(quarterly_activity_spark)\n",
    "    \n",
    "    # Column renaming mappings\n",
    "    column_mapping = {\n",
    "        'School': 'SchoolName',\n",
    "        \"# of Loans Originated\": \"ffel_subsidized_number_of_loans_originated\",\n",
    "        \"$ of Loans Originated\": \"ffel_subsidized_amount_of_loans_originated\",\n",
    "        \"Recipients\": \"ffel_subsidized_recipients\",\n",
    "        \"# of Loans Originated.1\": \"ffel_unsubsidized_number_of_loans_originated\",\n",
    "        \"$ of Loans Originated.1\": \"ffel_unsubsidized_amount_of_loans_originated\",\n",
    "        \"Recipients.1\": \"ffel_unsubsidized_recipients\",\n",
    "        \"# of Loans Originated.2\": \"ffel_stafford_number_of_loans_originated\",\n",
    "        \"$ of Loans Originated.2\": \"ffel_stafford_amount_of_loans_originated\",\n",
    "        \"Recipients.2\": \"ffel_stafford_recipients\",\n",
    "        \"# of Loans Originated.3\": \"ffel_plus_number_of_loans_originated\",\n",
    "        \"$ of Loans Originated.3\": \"ffel_plus_amount_of_loans_originated\",\n",
    "        \"Recipients.3\": \"ffel_plus_recipients\",\n",
    "        \"# of Disbursements\": \"ffel_subsidized_number_of_disbursements\",\n",
    "        \"$ of Disbursements\": \"ffel_subsidized_amount_of_disbursements\",\n",
    "        \"# of Disbursements.1\": \"ffel_unsubsidized_number_of_disbursements\",\n",
    "        \"$ of Disbursements.1\": \"ffel_unsubsidized_amount_of_disbursements\",\n",
    "        \"# of Disbursements.2\": \"ffel_stafford_number_of_disbursements\",\n",
    "        \"$ of Disbursements.2\": \"ffel_stafford_amount_of_disbursements\",\n",
    "        \"# of Disbursements.3\": \"ffel_plus_number_of_disbursements\",\n",
    "        \"$ of Disbursements.3\": \"ffel_plus_amount_of_disbursements\"\n",
    "    }\n",
    "    \n",
    "    # Rename columns\n",
    "    combined_df = rename_columns(combined_df, column_mapping)\n",
    "    \n",
    "    # Columns to cast to double\n",
    "    columns_to_cast = [\n",
    "        \"ffel_subsidized_amount_of_loans_originated\", \"ffel_unsubsidized_amount_of_loans_originated\",\n",
    "        \"ffel_stafford_amount_of_loans_originated\", \"ffel_plus_amount_of_loans_originated\",\n",
    "        \"ffel_subsidized_amount_of_disbursements\", \"ffel_unsubsidized_amount_of_disbursements\",\n",
    "        \"ffel_stafford_amount_of_disbursements\", \"ffel_plus_amount_of_disbursements\"\n",
    "    ]\n",
    "    \n",
    "    # Cast columns to double\n",
    "    combined_df = cast_columns_to_double(combined_df, columns_to_cast)\n",
    "    \n",
    "    # Save to Excel\n",
    "    save_to_excel(combined_df, output_path)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Process all files in raw directory\n",
    "def process_all_files(raw_dir, cleaned_dir):\n",
    "    spark = initialize_spark()\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(cleaned_dir):\n",
    "            os.makedirs(cleaned_dir)\n",
    "        \n",
    "        raw_files = [f for f in os.listdir(raw_dir) if f.endswith(\".xls\") or f.endswith(\".xlsx\")]\n",
    "        \n",
    "        for file_name in raw_files:\n",
    "            raw_file_path = os.path.join(raw_dir, file_name)\n",
    "            cleaned_file_path = os.path.join(cleaned_dir, f\"cleaned_{file_name}\")\n",
    "            \n",
    "            try:\n",
    "                process_ffel_data(raw_file_path, cleaned_file_path,spark)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "    finally:\n",
    "        # Stop Spark session\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load Fed School Code data and convert to Spark DataFrame\n",
    "def load_fed_school_code_data(file_path, spark):\n",
    "    # Customize based on the structure of the Fed School Code file\n",
    "    fed_school_code_pd = pd.read_excel(file_path)  # Adjust sheet name if different\n",
    "    fed_school_code_spark = spark.createDataFrame(fed_school_code_pd)\n",
    "    \n",
    "    return fed_school_code_spark\n",
    "\n",
    "\n",
    "# Main function to process Fed School Code data files\n",
    "def process_fed_school_code_data(file_path, output_path):\n",
    "    spark = initialize_spark()\n",
    "    try:\n",
    "        # Load Fed School Code data\n",
    "        fed_school_code_spark = load_fed_school_code_data(file_path, spark)\n",
    "        \n",
    "        # Save to Excel\n",
    "        save_to_excel(fed_school_code_spark, output_path)\n",
    "    \n",
    "    finally:\n",
    "        # Stop Spark session\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "file_path = '/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/raw'\n",
    "output_path ='/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned'\n",
    "fedschool_path ='/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/1617fedschoolcodelist.xls'\n",
    "fedschooloutput_path ='/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned_FFEL_data.xlsx'\n",
    "# process_ffel_data(file_path,output_path)\n",
    "\n",
    "process_all_files(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyhdfs\n",
    "import logging\n",
    "from config import hdfs_config\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Retrieve configuration\n",
    "HDFS_HOST = hdfs_config.HDFSConfig.HOST\n",
    "HDFS_PORT = hdfs_config.HDFSConfig.PORT\n",
    "LOCAL_RAW_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_RAW_DATA_PATH\n",
    "HDFS_RAW_DEST_PATH = hdfs_config.HDFSConfig.RAW_DEST_PATH\n",
    "LOCAL_CLEAN_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_CLEAN_DATA_PATH\n",
    "HDFS_CLEAN_DEST_PATH = hdfs_config.HDFSConfig.CLEAN_DEST_PATH\n",
    "\n",
    "def upload_files_to_hdfs(local_path, hdfs_path, hdfs_client):\n",
    "    \"\"\"Uploads files from a local directory to HDFS with detailed error handling and overwriting.\"\"\"\n",
    "    try:\n",
    "        # Ensure the HDFS directory exists\n",
    "        if not hdfs_client.exists(hdfs_path):\n",
    "            hdfs_client.mkdirs(hdfs_path)\n",
    "            logging.info(f\"Created HDFS directory: {hdfs_path}\")\n",
    "        \n",
    "        # Check if the local directory exists\n",
    "        if not os.path.exists(local_path):\n",
    "            logging.error(f\"Local directory {local_path} does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Iterate through files in the local directory\n",
    "        for file_name in os.listdir(local_path):\n",
    "            local_file_path = os.path.join(local_path, file_name)\n",
    "\n",
    "            # Only process files\n",
    "            if os.path.isfile(local_file_path):\n",
    "                hdfs_file_path = f\"{hdfs_path}/{file_name}\"\n",
    "\n",
    "                # Check if the file already exists in HDFS and remove it if it does\n",
    "                if hdfs_client.exists(hdfs_file_path):\n",
    "                    logging.info(f\"File {hdfs_file_path} already exists in HDFS. Deleting before upload.\")\n",
    "                    hdfs_client.delete(hdfs_file_path)\n",
    "\n",
    "                # Attempt to upload file to HDFS\n",
    "                try:\n",
    "                    with open(local_file_path, 'rb') as file_data:\n",
    "                        hdfs_client.create(hdfs_file_path, file_data)\n",
    "                    logging.info(f\"Uploaded {file_name} to HDFS at {hdfs_file_path}\")\n",
    "\n",
    "                    # Verify upload success\n",
    "                    if hdfs_client.exists(hdfs_file_path):\n",
    "                        logging.info(f\"Successfully uploaded: {file_name}\")\n",
    "                    else:\n",
    "                        logging.error(f\"Upload verification failed: {file_name}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to upload {file_name} to HDFS due to {e}\")\n",
    "\n",
    "            else:\n",
    "                logging.warning(f\"{local_file_path} is not a file. Skipping.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during file upload: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def combine_excel_files(folder_path, output_directory, output_filename=\"combined_data.xlsx\"):\n",
    "    \n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    # Lister tous les fichiers .xls et .xlsx dans le dossier\n",
    "    excel_files = [f for f in os.listdir(folder_path) if f.endswith('.xls') or f.endswith('.xlsx')]\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"Aucun fichier Excel trouvé dans le dossier : {folder_path}\")\n",
    "        return  # Sortir de la fonction\n",
    "    \n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    for file_name in excel_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            # Lire le fichier Excel\n",
    "            df = pd.read_excel(file_path, dtype={'OPE ID': str})\n",
    "            df['Timestamp'] = current_timestamp\n",
    "            # Combiner avec le DataFrame existant\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            print(f\"Fichier lu et combiné : {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Échec de la lecture du fichier {file_name} : {e}\")\n",
    "    \n",
    "    # Supprimer les lignes en doublon\n",
    "    combined_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Créer le répertoire de sortie s'il n'existe pas\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_directory, output_filename)\n",
    "    \n",
    "    try :\n",
    "        combined_df.to_excel(output_file_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Échec de l'enregistrement des données combinées dans le fichier {output_file_path} : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def combine_excel_files_parquet(folder_path, output_directory, output_filename=\"combined_data.parquet\"):\n",
    "    \n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    # Lister tous les fichiers .xls et .xlsx dans le dossier\n",
    "    excel_files = [f for f in os.listdir(folder_path) if f.endswith('.xls') or f.endswith('.xlsx')]\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"Aucun fichier Excel trouvé dans le dossier : {folder_path}\")\n",
    "        return  # Sortir de la fonction\n",
    "    \n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    for file_name in excel_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            # Lire le fichier Excel\n",
    "            df = pd.read_excel(file_path, dtype={'OPE ID': str})\n",
    "            df['OPE ID'] = df['OPE ID'].str[:-2]\n",
    "            df['Timestamp'] = current_timestamp\n",
    "            # Combiner avec le DataFrame existant\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            print(f\"Fichier lu et combiné : {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Échec de la lecture du fichier {file_name} : {e}\")\n",
    "    \n",
    "    # Supprimer les lignes en doublon\n",
    "    combined_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Créer le répertoire de sortie s'il n'existe pas\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_directory, output_filename)\n",
    "    \n",
    "    # Enregistrer le DataFrame combiné dans le chemin de sortie au format Parquet\n",
    "    try:\n",
    "        combined_df.to_parquet(output_file_path, index=False)\n",
    "        print(f\"Données combinées enregistrées dans le fichier Parquet : {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Échec de l'enregistrement des données combinées dans le fichier {output_file_path} : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier lu et combiné : cleaned_FL_Dashboard_AY2009_2010_Q4.xlsx\n",
      "Fichier lu et combiné : cleaned_FL_Dashboard_AY2009_2010_Q2.xlsx\n",
      "Fichier lu et combiné : cleaned_FL_Dashboard_AY2009_2010_Q3.xlsx\n",
      "Fichier lu et combiné : cleaned_FL_Dashboard_AY2009_2010_Q1.xlsx\n",
      "Fichier lu et combiné : cleaned_FL_Dashboard_AY2009_2010_Q4.xlsx\n",
      "Fichier lu et combiné : cleaned_FL_Dashboard_AY2009_2010_Q2.xlsx\n",
      "Fichier lu et combiné : cleaned_FL_Dashboard_AY2009_2010_Q3.xlsx\n",
      "Fichier lu et combiné : cleaned_FL_Dashboard_AY2009_2010_Q1.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rm/47tt4n7s30533krtvlkg53340000gn/T/ipykernel_89165/1329257508.py\", line 5, in <module>\n",
      "    combine_excel_files_parquet(folder_path, output_directory_parquet)\n",
      "  File \"/var/folders/rm/47tt4n7s30533krtvlkg53340000gn/T/ipykernel_89165/2125281910.py\", line 40, in combine_excel_files_parquet\n",
      "    combined_df.to_parquet(output_file_path, index=False)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/util/_decorators.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 3113, in to_parquet\n",
      "    return to_parquet(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py\", line 476, in to_parquet\n",
      "    impl = get_engine(engine)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py\", line 63, in get_engine\n",
      "    return engine_class()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py\", line 163, in __init__\n",
      "    import_optional_dependency(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données combinées enregistrées dans le fichier Parquet : /Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/parquet/combined_data.parquet\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned\"\n",
    "output_directory = \"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/combined\"\n",
    "output_directory_parquet = \"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/parquet\"\n",
    "combine_excel_files(folder_path, output_directory)\n",
    "combine_excel_files_parquet(folder_path, output_directory_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:HDFS client initialized\n",
      "INFO:pyhdfs:GETFILESTATUS /user/anthonycormeaux/data/dfparquet user.name=anthonycormeaux localhost:9870\n",
      "INFO:pyhdfs:GETFILESTATUS /user/anthonycormeaux/data/dfparquet/combined_data.parquet user.name=anthonycormeaux localhost:9870\n",
      "INFO:root:File /user/anthonycormeaux/data/dfparquet/combined_data.parquet already exists in HDFS. Deleting before upload.\n",
      "INFO:pyhdfs:DELETE /user/anthonycormeaux/data/dfparquet/combined_data.parquet user.name=anthonycormeaux localhost:9870\n",
      "INFO:pyhdfs:CREATE /user/anthonycormeaux/data/dfparquet/combined_data.parquet user.name=anthonycormeaux localhost:9870\n",
      "INFO:root:Uploaded combined_data.parquet to HDFS at /user/anthonycormeaux/data/dfparquet/combined_data.parquet\n",
      "INFO:pyhdfs:GETFILESTATUS /user/anthonycormeaux/data/dfparquet/combined_data.parquet user.name=anthonycormeaux localhost:9870\n",
      "INFO:root:Successfully uploaded: combined_data.parquet\n"
     ]
    }
   ],
   "source": [
    "local_parquet = \"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/parquet\"\n",
    "hdfs_path = \"/user/anthonycormeaux/data/dfparquet\"\n",
    "\n",
    "try:\n",
    "    # Initialize HDFS client\n",
    "    hdfs_client = pyhdfs.HdfsClient(hosts=f\"{HDFS_HOST}:{HDFS_PORT}\", user_name='anthonycormeaux')\n",
    "    logging.info(\"HDFS client initialized\")\n",
    "    \n",
    "    # Start uploading files\n",
    "    # upload_files_to_hdfs(LOCAL_RAW_DATA_PATH, HDFS_RAW_DEST_PATH, hdfs_client)\n",
    "    # upload_files_to_hdfs(LOCAL_CLEAN_DATA_PATH, HDFS_CLEAN_DEST_PATH, hdfs_client)\n",
    "    # upload_files_to_hdfs(LOCAL_COMBINED_DATA_PATH, COMBINED_DEST_PATH, hdfs_client)\n",
    "    upload_files_to_hdfs(local_parquet, hdfs_path, hdfs_client)\n",
    "    \n",
    "except pyhdfs.HdfsException as he:\n",
    "    logging.error(f\"HDFS error: Failed to initialize HDFS client or perform operations due to {he}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Unexpected error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/combined/combined_data.xlsx\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "def read_files_from_hdfs(hdfs_directory_path, hdfs_host, hdfs_port, nrows=None):\n",
    "    \"\"\"\n",
    "    Reads each file in the specified HDFS directory into a separate DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        hdfs_directory_path (str): HDFS directory path containing files to read.\n",
    "        hdfs_host (str): HDFS host address.\n",
    "        hdfs_port (int): HDFS port number.\n",
    "        nrows (int, optional): Number of rows to read from each file (for memory efficiency).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with file names as keys and DataFrames as values.\n",
    "    \"\"\"\n",
    "    client = InsecureClient(f'http://{hdfs_host}:{hdfs_port}')\n",
    "    data_frames = {}\n",
    "    \n",
    "    try:\n",
    "        # List all files in the specified HDFS directory\n",
    "        files = client.list(hdfs_directory_path)\n",
    "        \n",
    "        for file_name in files:\n",
    "            file_path = f\"{hdfs_directory_path}/{file_name}\"\n",
    "            \n",
    "            # Read file content from HDFS\n",
    "            with client.read(file_path) as f:\n",
    "                file_content = f.read()\n",
    "            \n",
    "            # Use BytesIO for compatibility with pandas\n",
    "            file_data = BytesIO(file_content)\n",
    "            \n",
    "            # Load the data into a DataFrame and store it in the dictionary\n",
    "            data_frames[file_name] = pd.read_excel(file_data, nrows=nrows)\n",
    "            print(f\"Loaded file '{file_name}' into a DataFrame.\")\n",
    "        \n",
    "        return data_frames\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files from HDFS: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "data_frames = read_files_from_hdfs(HDFS_CLEAN_DEST_PATH, HDFS_HOST, HDFS_PORT, nrows=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames[\"cleaned_FL_Dashboard_AY2009_2010_Q3.xlsx\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/1617fedschoolcodelist.xls\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(r\"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/1617fedschoolcodelist.xls\")\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "topic_name = 'excel_data'\n",
    "\n",
    "nb = 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        value_serializer=json_serializer\n",
    "    )\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        message = {\n",
    "            \"SchoolCode\": row[\"SchoolCode\"], \n",
    "            \"SchoolName\": row[\"SchoolName\"],\n",
    "            \"Address\": row[\"Address\"],\n",
    "            \"City\": row[\"City\"],\n",
    "            \"StateCode\" : row[\"StateCode\"],\n",
    "            \"ZipCode\" : row[\"ZipCode\"],\n",
    "            \"Country\" : row[\"Country\"],\n",
    "            \"timestamp\": datetime.now().timestamp()\n",
    "        }\n",
    "        producer.send(topic_name, value=message)\n",
    "        print(f\"Message envoyé pour {row['SchoolCode']}: {message} {nb}\")\n",
    "        nb += 1\n",
    "\n",
    "    producer.flush()\n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#envoie de données de crédit vers kafka\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_excel(\"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/combined/combined_data.xlsx\")\n",
    "\n",
    "df = df.drop_duplicates(subset='SchoolName')\n",
    "\n",
    "colonnes = [\"SchoolName\", \"State\",\"Zip Code\", \"School Type\"]\n",
    "\n",
    "df = df[colonnes]\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "    \n",
    "\n",
    "topic_name = 'excel_data'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        value_serializer=json_serializer\n",
    "    )\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "            message = {\n",
    "                    \"SchoolName\": row[\"SchoolName\"], \n",
    "                    \"State\": row[\"State\"],\n",
    "                    \"Zip Code\": row[\"Zip Code\"],\n",
    "                    \"School Type\": row[\"School Type\"],\n",
    "                    \"timestamp\": datetime.now().timestamp(),\n",
    "                    \"ffel_subsidized_recipients\" : random.randint(100,5000),\n",
    "                    \"ffel_subsidized_number_of_loans_originated\" : random.randint(500,8000),\n",
    "                    \"ffel_subsidized_amount_of_loans_originated\" : random.randint(100000, 999999999),\n",
    "                    \"ffel_subsidized_number_of_disbursements\" : random.randint(1000, 16000),\n",
    "\n",
    "                    \"ffel_unsubsidized_recipients\" : random.randint(100,5000),\n",
    "                    \"ffel_unsubsidized_number_of_loans_originated\" : random.randint(500,8000),\n",
    "                    \"ffel_unsubsidized_amount_of_loans_originated\" : random.randint(100000, 999999999),\n",
    "                    \"ffel_unsubsidizednumber_of_disbursements\" : random.randint(1000, 16000),\n",
    "\n",
    "                    \"ffel_stafford_recipients\" : random.randint(100,5000),\n",
    "                    \"ffel_stafford_number_of_loans_originated\" : random.randint(500,8000),\n",
    "                    \"ffel_stafford_amount_of_loans_originated\" : random.randint(100000, 999999999),\n",
    "                    \"ffel_stafford_of_disbursements\" : random.randint(1000, 16000),\n",
    "\n",
    "                    \"fffel_plus_recipients\" : random.randint(100,5000),\n",
    "                    \"fffel_plus_number_of_loans_originated\" : random.randint(500,8000),\n",
    "                    \"fffel_plus_amount_of_loans_originated\" : random.randint(100000, 999999999),\n",
    "                    \"fffel_plus_of_disbursements\" : random.randint(1000, 16000),\n",
    "\n",
    "                    \"Quarter_Start\" : \"04/01/2010\",\n",
    "                    \"Quarter_End\" : \"06/30/2010\"\n",
    "                }\n",
    "            producer.send(topic_name, value=message)\n",
    "            print(f\"Message envoyé pour {row}\")\n",
    "\n",
    "\n",
    "    producer.flush()\n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Configurez le consommateur pour se connecter à Kafka et consommer les messages en JSON\n",
    "def save_to_hdfs(dataframe):\n",
    "    # Placeholder: replace this with actual code to save data to HDFS\n",
    "    print(\"Data saved to HDFS:\", dataframe)\n",
    "\n",
    "# Initialize Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'excel_data',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='my-group',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8')) if x else None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Load initial HDFS data\n",
    "hdfs_df = data_frames[\"cleaned_FL_Dashboard_AY2009_2010_Q3.xlsx\"]\n",
    "\n",
    "print(\"Waiting for messages...\")\n",
    "batch_data = []\n",
    "batch_size = 10000\n",
    "\n",
    "for message in consumer:\n",
    "    if message.value:\n",
    "        try:\n",
    "            data = message.value\n",
    "            batch_data.append(data)\n",
    "            \n",
    "            # Display incoming batch data\n",
    "            print(f\"Batch data received: {batch_data[:5]}\")  # Display first 5 records for brevity\n",
    "\n",
    "            # Process in batches\n",
    "            if len(batch_data) >= batch_size:\n",
    "                # Create a DataFrame for the batch\n",
    "                new_df = pd.DataFrame(batch_data)\n",
    "\n",
    "                # Check if SchoolCode exists, else use SchoolName as the key\n",
    "                if 'SchoolCode' in hdfs_df.columns:\n",
    "                    # Merge on SchoolCode if it exists in both DataFrames\n",
    "                    merged_df = pd.merge(hdfs_df, new_df, on=\"SchoolCode\", how=\"outer\", suffixes=('', '_new'))\n",
    "                else:\n",
    "                    # If SchoolCode is missing, use SchoolName as the key\n",
    "                    merged_df = pd.merge(hdfs_df, new_df, on=\"SchoolName\", how=\"outer\", suffixes=('', '_new'))\n",
    "\n",
    "                # Update missing values with data from the new batch\n",
    "                for col in new_df.columns:\n",
    "                    if col not in ['SchoolCode', 'SchoolName']:  # Skip the identifier column(s)\n",
    "                        col_new = f\"{col}_new\"\n",
    "                        # Check if the new column exists in the merged DataFrame; add it if it doesn't\n",
    "                        if col_new not in merged_df.columns:\n",
    "                            merged_df[col_new] = None  # Add the missing column with None values\n",
    "\n",
    "                        # Perform the update where applicable\n",
    "                        merged_df[col] = merged_df.apply(\n",
    "                            lambda row: row[col_new] if pd.isnull(row[col]) else row[col], axis=1\n",
    "                        )\n",
    "\n",
    "                # Drop the temporary columns used for merging\n",
    "                merged_df = merged_df.drop([col + '_new' for col in new_df.columns if col not in ['SchoolCode', 'SchoolName']], axis=1)\n",
    "\n",
    "                # Save updated data back to HDFS in bulk\n",
    "                save_to_hdfs(merged_df)\n",
    "\n",
    "                # Update the HDFS DataFrame reference\n",
    "                hdfs_df = merged_df\n",
    "\n",
    "                # Clear batch data and commit offsets\n",
    "                batch_data = []\n",
    "                consumer.commit()  # Commit offsets manually after processing the batch\n",
    "\n",
    "                # Pause before the next batch\n",
    "                # sleep(10)\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decoding error: {e}\")\n",
    "            print(f\"Invalid message: {message.value}\")\n",
    "    else:\n",
    "        print(\"Empty or null message ignored.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
