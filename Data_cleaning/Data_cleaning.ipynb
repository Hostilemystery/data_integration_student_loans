{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import hdfs_config\n",
    "\n",
    "HDFS_HOST = hdfs_config.HDFSConfig.HOST\n",
    "HDFS_PORT = hdfs_config.HDFSConfig.PORT\n",
    "LOCAL_RAW_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_RAW_DATA_PATH\n",
    "HDFS_RAW_DEST_PATH = hdfs_config.HDFSConfig.RAW_DEST_PATH\n",
    "HDFS_CLEAN_DEST_PATH = hdfs_config.HDFSConfig.CLEAN_DEST_PATH\n",
    "LOCAL_CLEAN_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_CLEAN_DATA_PATH\n",
    "\n",
    "LOCAL_COMBINED_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_COMBINED_DATA_PATH\n",
    "COMBINED_DEST_PATH = hdfs_config.HDFSConfig.COMBINED_DEST_PATH\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def initialize_spark():\n",
    "    return SparkSession.builder.appName(\"Data Cleaning\").getOrCreate()\n",
    "\n",
    "def extract_quarter_dates(file_path):\n",
    "    try:\n",
    "        metadata_df = pd.read_excel(file_path, sheet_name='Quarterly Activity', nrows=5)\n",
    "        metadata_text = \" \".join(metadata_df.astype(str).values.flatten())\n",
    "        quarter_dates_match = re.search(r\"\\((\\d{2}/\\d{2}/\\d{4})-(\\d{2}/\\d{2}/\\d{4})\\)\", metadata_text)\n",
    "        quarter_start = quarter_dates_match.group(1) if quarter_dates_match else None\n",
    "        quarter_end = quarter_dates_match.group(2) if quarter_dates_match else None\n",
    "        return quarter_start, quarter_end\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting quarter dates: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def load_ffel_data(file_path, spark):\n",
    "    try:\n",
    "        quarterly_activity_pd = pd.read_excel(file_path, sheet_name='Quarterly Activity', skiprows=5, dtype={'OPE ID': str})\n",
    "        quarterly_activity_spark = spark.createDataFrame(quarterly_activity_pd)\n",
    "        return quarterly_activity_spark #, award_year_summary_spark\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def add_quarter_dates(quarterly_df, quarter_start, quarter_end):\n",
    "    quarterly_df = quarterly_df.withColumn(\"Quarter_Start\", lit(quarter_start)).withColumn(\"Quarter_End\", lit(quarter_end))\n",
    "    return quarterly_df\n",
    "\n",
    "def combine_dataframes(*dfs):\n",
    "    combined_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        combined_df = combined_df.union(df)\n",
    "    return combined_df.dropDuplicates()\n",
    "\n",
    "def rename_columns(df, column_mapping):\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df.columns:\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "    return df\n",
    "\n",
    "def cast_columns_to_double(df, columns_to_cast):\n",
    "    for col_name in columns_to_cast:\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "    return df\n",
    "\n",
    "def save_to_excel(df, output_path):\n",
    "    try:\n",
    "        if output_path.endswith(\".xls\"):\n",
    "            output_path = output_path.replace(\".xls\", \".xlsx\")\n",
    "            print(f\"Converted output file to .xlsx format: {output_path}\")\n",
    "\n",
    "        df_pd = df.toPandas()\n",
    "        df_pd.to_excel(output_path, index=False, float_format=\"%.2f\")\n",
    "        print(f\"Saved cleaned data to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "\n",
    "def process_ffel_data(file_path, output_path,spark):\n",
    "    \n",
    "    \n",
    "    quarter_start, quarter_end = extract_quarter_dates(file_path)\n",
    "    \n",
    "    quarterly_activity_spark = load_ffel_data(file_path, spark)\n",
    "    if not quarterly_activity_spark :\n",
    "        print(f\"Skipping {file_path} due to load errors.\")\n",
    "        return\n",
    "    \n",
    "    quarterly_activity_spark = add_quarter_dates(\n",
    "        quarterly_activity_spark, quarter_start, quarter_end\n",
    "    )\n",
    "\n",
    "    \n",
    "    combined_df = combine_dataframes(quarterly_activity_spark)\n",
    "    \n",
    "    column_mapping = {\n",
    "        'School': 'SchoolName',\n",
    "        \"# of Loans Originated\": \"ffel_subsidized_number_of_loans_originated\",\n",
    "        \"$ of Loans Originated\": \"ffel_subsidized_amount_of_loans_originated\",\n",
    "        \"Recipients\": \"ffel_subsidized_recipients\",\n",
    "        \"# of Loans Originated.1\": \"ffel_unsubsidized_number_of_loans_originated\",\n",
    "        \"$ of Loans Originated.1\": \"ffel_unsubsidized_amount_of_loans_originated\",\n",
    "        \"Recipients.1\": \"ffel_unsubsidized_recipients\",\n",
    "        \"# of Loans Originated.2\": \"ffel_stafford_number_of_loans_originated\",\n",
    "        \"$ of Loans Originated.2\": \"ffel_stafford_amount_of_loans_originated\",\n",
    "        \"Recipients.2\": \"ffel_stafford_recipients\",\n",
    "        \"# of Loans Originated.3\": \"ffel_plus_number_of_loans_originated\",\n",
    "        \"$ of Loans Originated.3\": \"ffel_plus_amount_of_loans_originated\",\n",
    "        \"Recipients.3\": \"ffel_plus_recipients\",\n",
    "        \"# of Disbursements\": \"ffel_subsidized_number_of_disbursements\",\n",
    "        \"$ of Disbursements\": \"ffel_subsidized_amount_of_disbursements\",\n",
    "        \"# of Disbursements.1\": \"ffel_unsubsidized_number_of_disbursements\",\n",
    "        \"$ of Disbursements.1\": \"ffel_unsubsidized_amount_of_disbursements\",\n",
    "        \"# of Disbursements.2\": \"ffel_stafford_number_of_disbursements\",\n",
    "        \"$ of Disbursements.2\": \"ffel_stafford_amount_of_disbursements\",\n",
    "        \"# of Disbursements.3\": \"ffel_plus_number_of_disbursements\",\n",
    "        \"$ of Disbursements.3\": \"ffel_plus_amount_of_disbursements\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    combined_df = rename_columns(combined_df, column_mapping)\n",
    "    \n",
    "    columns_to_cast = [\n",
    "        \"ffel_subsidized_amount_of_loans_originated\", \"ffel_unsubsidized_amount_of_loans_originated\",\n",
    "        \"ffel_stafford_amount_of_loans_originated\", \"ffel_plus_amount_of_loans_originated\",\n",
    "        \"ffel_subsidized_amount_of_disbursements\", \"ffel_unsubsidized_amount_of_disbursements\",\n",
    "        \"ffel_stafford_amount_of_disbursements\", \"ffel_plus_amount_of_disbursements\"\n",
    "    ]\n",
    "    \n",
    "    combined_df = cast_columns_to_double(combined_df, columns_to_cast)\n",
    "    \n",
    "    save_to_excel(combined_df, output_path)\n",
    "    \n",
    "    \n",
    "\n",
    "def process_all_files(raw_dir, cleaned_dir):\n",
    "    spark = initialize_spark()\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(cleaned_dir):\n",
    "            os.makedirs(cleaned_dir)\n",
    "        \n",
    "        raw_files = [f for f in os.listdir(raw_dir) if f.endswith(\".xls\") or f.endswith(\".xlsx\")]\n",
    "        \n",
    "        for file_name in raw_files:\n",
    "            raw_file_path = os.path.join(raw_dir, file_name)\n",
    "            cleaned_file_path = os.path.join(cleaned_dir, f\"cleaned_{file_name}\")\n",
    "            \n",
    "            try:\n",
    "                process_ffel_data(raw_file_path, cleaned_file_path,spark)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "def load_fed_school_code_data(file_path, spark):\n",
    "    fed_school_code_pd = pd.read_excel(file_path)\n",
    "    fed_school_code_spark = spark.createDataFrame(fed_school_code_pd)\n",
    "    \n",
    "    return fed_school_code_spark\n",
    "\n",
    "\n",
    "def process_fed_school_code_data(file_path, output_path):\n",
    "    spark = initialize_spark()\n",
    "    try:\n",
    "        fed_school_code_spark = load_fed_school_code_data(file_path, spark)\n",
    "        \n",
    "        save_to_excel(fed_school_code_spark, output_path)\n",
    "    \n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "file_path = '/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/raw'\n",
    "output_path ='/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned'\n",
    "\n",
    "process_all_files(file_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
