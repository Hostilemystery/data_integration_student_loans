{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyhdfs\n",
    "import logging\n",
    "from config import hdfs_config\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "HDFS_HOST = hdfs_config.HDFSConfig.HOST\n",
    "HDFS_PORT = hdfs_config.HDFSConfig.PORT\n",
    "LOCAL_RAW_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_RAW_DATA_PATH\n",
    "HDFS_RAW_DEST_PATH = hdfs_config.HDFSConfig.RAW_DEST_PATH\n",
    "LOCAL_CLEAN_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_CLEAN_DATA_PATH\n",
    "HDFS_CLEAN_DEST_PATH = hdfs_config.HDFSConfig.CLEAN_DEST_PATH\n",
    "\n",
    "LOCAL_COMBINED_DATA_PATH = hdfs_config.HDFSConfig.LOCAL_COMBINED_DATA_PATH\n",
    "COMBINED_DEST_PATH = hdfs_config.HDFSConfig.COMBINED_DEST_PATH\n",
    "\n",
    "def upload_files_to_hdfs(local_path, hdfs_path, hdfs_client):\n",
    "    try:\n",
    "        if not hdfs_client.exists(hdfs_path):\n",
    "            hdfs_client.mkdirs(hdfs_path)\n",
    "            logging.info(f\"Created HDFS directory: {hdfs_path}\")\n",
    "        \n",
    "        if not os.path.exists(local_path):\n",
    "            logging.error(f\"Local directory {local_path} does not exist.\")\n",
    "            return\n",
    "\n",
    "        for file_name in os.listdir(local_path):\n",
    "            local_file_path = os.path.join(local_path, file_name)\n",
    "\n",
    "            if os.path.isfile(local_file_path):\n",
    "                hdfs_file_path = f\"{hdfs_path}/{file_name}\"\n",
    "\n",
    "                if hdfs_client.exists(hdfs_file_path):\n",
    "                    logging.info(f\"File {hdfs_file_path} already exists in HDFS. Deleting before upload.\")\n",
    "                    hdfs_client.delete(hdfs_file_path)\n",
    "\n",
    "                try:\n",
    "                    with open(local_file_path, 'rb') as file_data:\n",
    "                        hdfs_client.create(hdfs_file_path, file_data)\n",
    "                    logging.info(f\"Uploaded {file_name} to HDFS at {hdfs_file_path}\")\n",
    "\n",
    "                    if hdfs_client.exists(hdfs_file_path):\n",
    "                        logging.info(f\"Successfully uploaded: {file_name}\")\n",
    "                    else:\n",
    "                        logging.error(f\"Upload verification failed: {file_name}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to upload {file_name} to HDFS due to {e}\")\n",
    "\n",
    "            else:\n",
    "                logging.warning(f\"{local_file_path} is not a file. Skipping.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during file upload: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def combine_excel_files_parquet(folder_path, output_directory, output_filename=\"combined_data.parquet\"):\n",
    "    \n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    excel_files = [f for f in os.listdir(folder_path) if f.endswith('.xls') or f.endswith('.xlsx')]\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"Aucun fichier Excel trouvé dans le dossier : {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    for file_name in excel_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, dtype={'OPE ID': str})\n",
    "            df['OPE ID'] = df['OPE ID'].str[:-2]\n",
    "            df['Timestamp'] = current_timestamp\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            print(f\"Fichier lu et combiné : {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Échec de la lecture du fichier {file_name} : {e}\")\n",
    "    \n",
    "    combined_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "    try:\n",
    "        combined_df.to_parquet(output_file_path, index=False)\n",
    "        print(f\"Données combinées enregistrées dans le fichier Parquet : {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Échec de l'enregistrement des données combinées dans le fichier {output_file_path} : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/cleaned\"\n",
    "output_directory_parquet = \"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/parquet\"\n",
    "combine_excel_files_parquet(folder_path, output_directory_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_parquet = \"/Users/anthonycormeaux/Documents/Projet_data_integration/Nouvelle version/data_integration_student_loans/data/parquet\"\n",
    "hdfs_path = \"/user/anthonycormeaux/data/dfparquet\"\n",
    "\n",
    "try:\n",
    "    hdfs_client = pyhdfs.HdfsClient(hosts=f\"{HDFS_HOST}:{HDFS_PORT}\", user_name='anthonycormeaux')\n",
    "    logging.info(\"HDFS client initialized\")\n",
    "\n",
    "    upload_files_to_hdfs(local_parquet, hdfs_path, hdfs_client)\n",
    "    \n",
    "except pyhdfs.HdfsException as he:\n",
    "    logging.error(f\"HDFS error: Failed to initialize HDFS client or perform operations due to {he}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Unexpected error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
