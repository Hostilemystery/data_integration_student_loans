{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import logging\n",
    "import shutil\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,io.delta:delta-core_2.12:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer le logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"KafkaToHDFSUpdater\")\n",
    "\n",
    "# Initialiser la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToHDFSUpdater\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schéma du fichier CSV\n",
    "csv_schema = StructType([\n",
    "    StructField(\"OPE ID\", StringType(), True),\n",
    "    StructField(\"SchoolName\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Zip Code\", StringType(), True),\n",
    "    StructField(\"School Type\", StringType(), True),\n",
    "    StructField(\"ffel_subsidized_recipients\", IntegerType(), True),\n",
    "    StructField(\"ffel_subsidized_number_of_loans_originated\", IntegerType(), True),\n",
    "    StructField(\"ffel_subsidized_amount_of_loans_originated\", DoubleType(), True),\n",
    "    StructField(\"ffel_subsidized_number_of_disbursements\", IntegerType(), True),\n",
    "    StructField(\"ffel_subsidized_amount_of_disbursements\", DoubleType(), True),\n",
    "    StructField(\"ffel_unsubsidized_recipients\", IntegerType(), True),\n",
    "    StructField(\"ffel_unsubsidized_number_of_loans_originated\", IntegerType(), True),\n",
    "    StructField(\"ffel_unsubsidized_amount_of_loans_originated\", DoubleType(), True),\n",
    "    StructField(\"ffel_unsubsidized_number_of_disbursements\", IntegerType(), True),\n",
    "    StructField(\"ffel_unsubsidized_amount_of_disbursements\", DoubleType(), True),\n",
    "    StructField(\"ffel_stafford_recipients\", IntegerType(), True),\n",
    "    StructField(\"ffel_stafford_number_of_loans_originated\", IntegerType(), True),\n",
    "    StructField(\"ffel_stafford_amount_of_loans_originated\", DoubleType(), True),\n",
    "    StructField(\"ffel_stafford_number_of_disbursements\", IntegerType(), True),\n",
    "    StructField(\"ffel_stafford_amount_of_disbursements\", DoubleType(), True),\n",
    "    StructField(\"ffel_plus_recipients\", IntegerType(), True),\n",
    "    StructField(\"ffel_plus_number_of_loans_originated\", IntegerType(), True),\n",
    "    StructField(\"ffel_plus_amount_of_loans_originated\", DoubleType(), True),\n",
    "    StructField(\"ffel_plus_number_of_disbursements\", IntegerType(), True),\n",
    "    StructField(\"ffel_plus_amount_of_disbursements\", DoubleType(), True),\n",
    "    StructField(\"Quarter_Start\", StringType(), True),  # Utiliser StringType ou DateType selon le format\n",
    "    StructField(\"Quarter_End\", StringType(), True),    # Utiliser StringType ou DateType selon le format\n",
    "    StructField(\"Timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Schéma des messages Kafka\n",
    "kafka_schema = StructType([\n",
    "    StructField(\"SchoolCode\", StringType(), True),\n",
    "    StructField(\"SchoolName\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"StateCode\", StringType(), True),\n",
    "    StructField(\"ZipCode\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"timestamp\", DoubleType(), True)  # Timestamp en secondes depuis epoch\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"hdfs://localhost:9080/user/anthonycormeaux/data/combined/combined_data.csv\"\n",
    "\n",
    "csv_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(csv_schema) \\\n",
    "    .csv(csv_path) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = \"localhost:9092\"  # Utiliser l'adresse IP directement\n",
    "kafka_topic = \"excel_data\"\n",
    "\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), kafka_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"message_timestamp\", (col(\"timestamp\") / 1000).cast(TimestampType()))  # Convertir en Timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_csv(batch_df, batch_id):\n",
    "    global csv_df\n",
    "    logger.info(f\"Processing batch_id: {batch_id}\")\n",
    "    \n",
    "    # Joindre le batch Kafka avec le CSV en mémoire sur 'SchoolName'\n",
    "    updated_df = csv_df.alias(\"csv\") \\\n",
    "        .join(batch_df.alias(\"kafka\"), on=\"SchoolName\", how=\"left\") \\\n",
    "        .select(\n",
    "            when(\n",
    "                (col(\"kafka.SchoolCode\").isNotNull()) & (col(\"kafka.message_timestamp\") > col(\"csv.Timestamp\")),\n",
    "                col(\"kafka.SchoolCode\")\n",
    "            ).otherwise(col(\"csv.OPE ID\")).alias(\"OPE ID\"),\n",
    "            \n",
    "            col(\"csv.SchoolName\"),\n",
    "            \n",
    "            when(\n",
    "                (col(\"kafka.StateCode\").isNotNull()) & (col(\"kafka.message_timestamp\") > col(\"csv.Timestamp\")),\n",
    "                col(\"kafka.StateCode\")\n",
    "            ).otherwise(col(\"csv.State\")).alias(\"State\"),\n",
    "            \n",
    "            when(\n",
    "                (col(\"kafka.ZipCode\").isNotNull()) & (col(\"kafka.message_timestamp\") > col(\"csv.Timestamp\")),\n",
    "                col(\"kafka.ZipCode\").cast(StringType())\n",
    "            ).otherwise(col(\"csv.Zip Code\")).alias(\"Zip Code\"),\n",
    "            \n",
    "            col(\"csv.School Type\"),\n",
    "            \n",
    "            col(\"csv.ffel_subsidized_recipients\"),\n",
    "            col(\"csv.ffel_subsidized_number_of_loans_originated\"),\n",
    "            col(\"csv.ffel_subsidized_amount_of_loans_originated\"),\n",
    "            col(\"csv.ffel_subsidized_number_of_disbursements\"),\n",
    "            col(\"csv.ffel_subsidized_amount_of_disbursements\"),\n",
    "            col(\"csv.ffel_unsubsidized_recipients\"),\n",
    "            col(\"csv.ffel_unsubsidized_number_of_loans_originated\"),\n",
    "            col(\"csv.ffel_unsubsidized_amount_of_loans_originated\"),\n",
    "            col(\"csv.ffel_unsubsidized_number_of_disbursements\"),\n",
    "            col(\"csv.ffel_unsubsidized_amount_of_disbursements\"),\n",
    "            col(\"csv.ffel_stafford_recipients\"),\n",
    "            col(\"csv.ffel_stafford_number_of_loans_originated\"),\n",
    "            col(\"csv.ffel_stafford_amount_of_loans_originated\"),\n",
    "            col(\"csv.ffel_stafford_number_of_disbursements\"),\n",
    "            col(\"csv.ffel_stafford_amount_of_disbursements\"),\n",
    "            col(\"csv.ffel_plus_recipients\"),\n",
    "            col(\"csv.ffel_plus_number_of_loans_originated\"),\n",
    "            col(\"csv.ffel_plus_amount_of_loans_originated\"),\n",
    "            col(\"csv.ffel_plus_number_of_disbursements\"),\n",
    "            col(\"csv.ffel_plus_amount_of_disbursements\"),\n",
    "            \n",
    "            col(\"csv.Quarter_Start\"),\n",
    "            col(\"csv.Quarter_End\"),\n",
    "            \n",
    "            when(\n",
    "                (col(\"kafka.message_timestamp\").isNotNull()) & (col(\"kafka.message_timestamp\") > col(\"csv.Timestamp\")),\n",
    "                col(\"kafka.message_timestamp\")\n",
    "            ).otherwise(col(\"csv.Timestamp\")).alias(\"Timestamp\")\n",
    "        )\n",
    "    \n",
    "    # Mettre à jour le DataFrame en mémoire\n",
    "    csv_df = updated_df.cache()\n",
    "    \n",
    "    # Afficher le nombre de lignes mises à jour\n",
    "    count = updated_df.count()\n",
    "    logger.info(f\"Updated CSV with {count} rows\")\n",
    "    \n",
    "    # Réduire à une seule partition pour obtenir un seul fichier\n",
    "    single_partition_df = updated_df.coalesce(1)\n",
    "    \n",
    "    # Chemin temporaire pour l'écriture\n",
    "    temp_output_path = \"hdfs://localhost:9080/user/anthonycormeaux/data/temp\"\n",
    "    \n",
    "    # Écrire le CSV mis à jour sur HDFS en mode overwrite dans le répertoire temporaire\n",
    "    single_partition_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_output_path)\n",
    "    \n",
    "    logger.info(f\"Batch {batch_id} written to temporary HDFS directory\")\n",
    "    \n",
    "    # Déplacer le fichier depuis le répertoire temporaire vers le chemin final\n",
    "    # Cela implique de supprimer l'ancien fichier et de renommer le nouveau\n",
    "    # Utilisez les commandes Hadoop pour cela\n",
    "    \n",
    "    # Obtenir le nom du fichier écrit\n",
    "    hdfs_ls_output = subprocess.check_output([\"hdfs\", \"dfs\", \"-ls\", temp_output_path]).decode(\"utf-8\")\n",
    "    lines = hdfs_ls_output.strip().split(\"\\n\")\n",
    "    if len(lines) < 2:\n",
    "        logger.error(\"Aucun fichier CSV trouvé dans le répertoire temporaire.\")\n",
    "        return\n",
    "    # Supposer que le fichier CSV est le second élément\n",
    "    temp_csv_file = lines[1].split()[-1]\n",
    "    \n",
    "    final_csv_path = \"hdfs://localhost:9080/user/anthonycormeaux/data/combined/combined_data.csv\"\n",
    "    \n",
    "    # Supprimer l'ancien fichier CSV\n",
    "    subprocess.call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", final_csv_path])\n",
    "    \n",
    "    # Déplacer le nouveau fichier CSV vers le chemin final\n",
    "    subprocess.call([\"hdfs\", \"dfs\", \"-mv\", temp_csv_file, final_csv_path])\n",
    "    \n",
    "    # Supprimer le répertoire temporaire\n",
    "    subprocess.call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", temp_output_path])\n",
    "    \n",
    "    logger.info(f\"Batch {batch_id} moved to final HDFS CSV file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:KafkaToHDFSUpdater:Processing batch_id: 2\n",
      "INFO:KafkaToHDFSUpdater:Updated CSV with 3702304 rows                           \n",
      "INFO:KafkaToHDFSUpdater:Batch 2 written to temporary HDFS directory             \n",
      "2024-11-20 21:33:15,580 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-11-20 21:33:17,057 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `hdfs://localhost:9080/user/anthonycormeaux/data/combined/combined_data.csv': No such file or directory\n",
      "2024-11-20 21:33:18,492 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-11-20 21:33:20,111 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "INFO:KafkaToHDFSUpdater:Batch 2 moved to final HDFS CSV file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://localhost:9080/user/anthonycormeaux/data/temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:KafkaToHDFSUpdater:Processing batch_id: 3\n",
      "24/11/20 21:34:03 WARN MemoryStore: Not enough space to cache rdd_101_166 in memory! (computed 10.4 MiB so far)\n",
      "24/11/20 21:34:03 WARN BlockManager: Persisting block rdd_101_166 to disk instead.\n",
      "24/11/20 21:34:03 WARN MemoryStore: Not enough space to cache rdd_101_60 in memory! (computed 31.1 MiB so far)\n",
      "24/11/20 21:34:03 WARN BlockManager: Persisting block rdd_101_60 to disk instead.\n",
      "24/11/20 21:34:04 WARN MemoryStore: Not enough space to cache rdd_101_111 in memory! (computed 31.3 MiB so far)\n",
      "24/11/20 21:34:04 WARN BlockManager: Persisting block rdd_101_111 to disk instead.\n",
      "24/11/20 21:34:04 WARN MemoryStore: Not enough space to cache rdd_101_141 in memory! (computed 41.6 MiB so far)\n",
      "24/11/20 21:34:04 WARN BlockManager: Persisting block rdd_101_141 to disk instead.\n",
      "24/11/20 21:34:09 WARN MemoryStore: Not enough space to cache rdd_101_111 in memory! (computed 31.3 MiB so far)\n",
      "24/11/20 21:34:09 WARN MemoryStore: Not enough space to cache rdd_101_60 in memory! (computed 52.6 MiB so far)\n",
      "24/11/20 21:34:11 WARN MemoryStore: Not enough space to cache rdd_101_141 in memory! (computed 62.9 MiB so far)\n",
      "24/11/20 21:36:51 WARN MemoryStore: Not enough space to cache rdd_101_166 in memory! (computed 167.6 MiB so far)\n",
      "24/11/20 21:36:51 WARN MemoryStore: Not enough space to cache rdd_101_141 in memory! (computed 41.6 MiB so far)\n",
      "24/11/20 21:36:51 WARN MemoryStore: Not enough space to cache rdd_101_60 in memory! (computed 83.7 MiB so far)\n",
      "24/11/20 21:36:51 WARN MemoryStore: Not enough space to cache rdd_101_166 in memory! (computed 39.9 MiB so far)\n",
      "24/11/20 21:36:51 WARN MemoryStore: Not enough space to cache rdd_101_111 in memory! (computed 84.0 MiB so far)\n",
      "INFO:KafkaToHDFSUpdater:Updated CSV with 101435068 rows                         \n",
      "24/11/20 21:37:03 WARN MemoryStore: Not enough space to cache rdd_101_111 in memory! (computed 52.3 MiB so far)\n",
      "24/11/20 21:37:08 WARN MemoryStore: Not enough space to cache rdd_101_141 in memory! (computed 41.6 MiB so far)\n",
      "24/11/20 21:37:16 WARN MemoryStore: Not enough space to cache rdd_101_166 in memory! (computed 39.9 MiB so far)\n",
      "INFO:KafkaToHDFSUpdater:Batch 3 written to temporary HDFS directory             \n",
      "2024-11-20 21:40:23,400 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-11-20 21:40:24,792 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://localhost:9080/user/anthonycormeaux/data/combined/combined_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 21:40:26,121 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-11-20 21:40:27,415 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "INFO:KafkaToHDFSUpdater:Batch 3 moved to final HDFS CSV file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://localhost:9080/user/anthonycormeaux/data/temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 21:40:28 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 60000 milliseconds, but spent 388495 milliseconds\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:KafkaToHDFSUpdater:Processing batch_id: 4\n",
      "24/11/20 21:40:30 WARN MemoryStore: Not enough space to cache rdd_101_111 in memory! (computed 52.3 MiB so far)\n",
      "24/11/20 21:40:30 WARN MemoryStore: Not enough space to cache rdd_101_120 in memory! (computed 11.1 MiB so far)\n",
      "24/11/20 21:40:30 WARN MemoryStore: Not enough space to cache rdd_101_141 in memory! (computed 10.4 MiB so far)\n",
      "24/11/20 21:40:31 WARN MemoryStore: Not enough space to cache rdd_129_141 in memory! (computed 10.4 MiB so far)\n",
      "24/11/20 21:40:31 WARN BlockManager: Persisting block rdd_129_141 to disk instead.\n",
      "24/11/20 21:40:31 WARN MemoryStore: Not enough space to cache rdd_129_146 in memory! (computed 11.0 MiB so far)\n",
      "24/11/20 21:40:31 WARN BlockManager: Persisting block rdd_129_146 to disk instead.\n",
      "24/11/20 21:40:31 WARN MemoryStore: Not enough space to cache rdd_129_55 in memory! (computed 21.1 MiB so far)\n",
      "24/11/20 21:40:31 WARN BlockManager: Persisting block rdd_129_55 to disk instead.\n",
      "24/11/20 21:40:31 WARN MemoryStore: Not enough space to cache rdd_101_166 in memory! (computed 39.9 MiB so far)\n",
      "24/11/20 21:40:31 WARN MemoryStore: Not enough space to cache rdd_129_111 in memory! (computed 20.3 MiB so far)\n",
      "24/11/20 21:40:31 WARN BlockManager: Persisting block rdd_129_111 to disk instead.\n",
      "24/11/20 21:40:32 WARN MemoryStore: Not enough space to cache rdd_101_195 in memory! (computed 10.9 MiB so far)\n",
      "24/11/20 21:40:32 WARN MemoryStore: Not enough space to cache rdd_129_166 in memory! (computed 10.4 MiB so far)\n",
      "24/11/20 21:40:32 WARN BlockManager: Persisting block rdd_129_166 to disk instead.\n",
      "24/11/20 21:40:33 WARN MemoryStore: Not enough space to cache rdd_129_55 in memory! (computed 52.7 MiB so far)\n",
      "24/11/20 21:40:38 WARN MemoryStore: Not enough space to cache rdd_129_60 in memory! (computed 136.2 MiB so far)\n",
      "24/11/20 21:40:38 WARN BlockManager: Persisting block rdd_129_60 to disk instead.\n",
      "24/11/20 21:40:39 WARN MemoryStore: Not enough space to cache rdd_129_111 in memory! (computed 136.2 MiB so far)\n",
      "24/11/20 21:40:49 WARN MemoryStore: Not enough space to cache rdd_129_141 in memory! (computed 144.9 MiB so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_166 in memory! (computed 167.6 MiB so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_149 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_149 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_150 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_150 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_151 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_152 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_151 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_152 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_153 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_153 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_154 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_141 in memory! (computed 10.4 MiB so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_154 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_146 in memory! (computed 11.0 MiB so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_156 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_155 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_156 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_155 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_161 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_161 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_164 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_164 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_163 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_163 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_165 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_111 in memory! (computed 52.3 MiB so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_165 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_168 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_167 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_129_166 in memory.\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_167 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_168 in memory! (computed 384.0 B so far)\n",
      "24/11/20 21:57:40 WARN MemoryStore: Not enough space to cache rdd_129_166 in memory! (computed 384.0 B so far)\n",
      "INFO:KafkaToHDFSUpdater:Updated CSV with 553051054 rows                         \n",
      "24/11/20 21:58:07 WARN MemoryStore: Not enough space to cache rdd_129_111 in memory! (computed 31.3 MiB so far)\n",
      "24/11/20 21:58:14 WARN MemoryStore: Not enough space to cache rdd_129_141 in memory! (computed 39.9 MiB so far)\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"hdfs://localhost:9080/user/anthonycormeaux/data/checkpoint\"\n",
    "\n",
    "query = parsed_kafka_df.writeStream \\\n",
    "    .foreachBatch(update_csv) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime='1 minute') \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
